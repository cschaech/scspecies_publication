{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import muon as mu\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import os\n",
    "import random\n",
    "from models import scSpecies\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from create_datasets.preprocessing import set_random_seed\n",
    "from create_datasets.preprocessing import create_mdata\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "set_random_seed(1234)\n",
    "\n",
    "path = os.path.abspath('').replace('\\\\', '/')+'/'\n",
    "data_path = path+'dataset/'\n",
    "save_path = os.path.abspath('').replace('\\\\', '/')+'/results/'\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dict_list = [\n",
    "                {'k_neigh': 25, 'alignment': 'latent'}, \n",
    "                {'k_neigh': 1, 'alignment': 'latent'}, \n",
    "                {'k_neigh': 250, 'alignment': 'latent'}, \n",
    "                {'k_neigh': 25, 'alignment': 'inter'}\n",
    "              ]\n",
    "\n",
    "dataset_list = [\n",
    "                {'dataset': \"liver_human\", 'context_key': 'mouse', 'target_key': 'human', 'load_key': 'liver'},  \n",
    "                {'dataset': \"glio\", 'context_key': 'mouse', 'target_key': 'human', 'load_key': 'glio'}, \n",
    "                {'dataset': \"adipose\", 'context_key': 'mouse', 'target_key': 'human', 'load_key': 'adipose'}, \n",
    "                {'dataset': \"liver_mouseNafld\", 'context_key': 'mouse', 'target_key': 'mouseNafld', 'load_key': 'liver'},                \n",
    "               ]\n",
    "\n",
    "for dataset_dict in dataset_list:\n",
    "    dataset = dataset_dict['dataset']\n",
    "    context_key = dataset_dict['context_key']\n",
    "    target_key = dataset_dict['target_key']\n",
    "    load_key = dataset_dict['load_key']\n",
    "\n",
    "    for params in h_dict_list:\n",
    "\n",
    "        k_neigh = params['k_neigh']\n",
    "        alignment = params['alignment'] \n",
    "        \n",
    "        for i in range(10):\n",
    "            save_key = load_key +'_'+ context_key +'_'+ target_key +'_'+ str(k_neigh) +'_'+ alignment +'_'+ str(i) \n",
    "\n",
    "            print(dataset, i, k_neigh, alignment)\n",
    "\n",
    "            mdata = mu.read_h5mu(path+f\"dataset/{load_key}.h5mu\")\n",
    "\n",
    "            model = scSpecies(device, \n",
    "                              mdata, \n",
    "                              path, \n",
    "                              k_neigh = k_neigh,\n",
    "                              alignment = alignment,\n",
    "                              context_dataset_key = context_key, \n",
    "                              target_dataset_key = target_key, \n",
    "                              random_seed = 1234*i\n",
    "                              )\n",
    "\n",
    "            model.train_context(30, save_model=False)\n",
    "            model.eval_context()\n",
    "\n",
    "            if dataset == \"liver_human\" and k_neigh == 25 and alignment == 'latent':\n",
    "                model.train_target(30, save_model=False, track_prototypes=True)\n",
    "                model.eval_target()    \n",
    "                np.save(path+'save_adata/'+save_key+\"_sim_metric.npy\", np.array(model.sim_metric))\n",
    "\n",
    "            else:\n",
    "                model.train_target(30, save_model=False)\n",
    "                model.eval_target()  \n",
    "\n",
    "            model.save_params(save_key, save='both', name='')\n",
    "\n",
    "            model.eval_label_transfer([('cell_type_coarse', 'cell_type_coarse'), ('cell_type_fine', 'cell_type_fine')])\n",
    "\n",
    "            #adata = ad.AnnData(np.concat([model.mdata[context_key].obsm['latent_mu'], model.mdata[target_key].obsm['latent_mu']]))\n",
    "            #obs = pd.concat([model.mdata[context_key].obs, model.mdata[target_key].obs])\n",
    "            #obs['species'] = np.array(['context'] * model.mdata[context_key].n_obs + ['target'] * model.mdata[target_key].n_obs)\n",
    "            #adata.obs = obs\n",
    "\n",
    "            #adata.uns['ind_nns_aligned_latent_space'] = model.mdata[target_key].obsm['ind_nns_aligned_latent_space']\n",
    "            #adata.uns['nlog_likeli_nns_aligned_latent_space'] = model.mdata[target_key].obsm['nlog_likeli_nns_aligned_latent_space']\n",
    "            #adata.write(path+'save_adata/'+save_key+'.h5ad')\n",
    "            model.mdata.write(save_path+'/'+save_key+'.h5mu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dict_list = [\n",
    "                {'k_neigh': 25, 'alignment': 'latent'}, \n",
    "              ]\n",
    "\n",
    "dataset_list = [\n",
    "                {'dataset': \"liver_human\", 'context_key': 'mouse', 'target_key': 'human', 'load_key': 'liver'},                 \n",
    "               ]\n",
    "\n",
    "for dataset_size in [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 20000, 50000]:\n",
    "\n",
    "    for dataset_dict in dataset_list:\n",
    "        dataset = dataset_dict['dataset']\n",
    "        context_key = dataset_dict['context_key']\n",
    "        target_key = dataset_dict['target_key']\n",
    "        load_key = dataset_dict['load_key']\n",
    "\n",
    "        for params in h_dict_list:\n",
    "\n",
    "            k_neigh = params['k_neigh']\n",
    "            alignment = params['alignment'] \n",
    "            \n",
    "            for i in range(10):\n",
    "                save_key = load_key +'_'+ context_key +'_'+ target_key +'_'+ str(k_neigh) +'_'+ alignment +'_'+ str(i) +'_size_'+ str(dataset_size)\n",
    "\n",
    "                mdata = mu.read_h5mu(path+f\"dataset/{load_key}.h5mu\")\n",
    "\n",
    "                sc.pp.subsample(mdata.mod['human'], n_obs=dataset_size, random_state=1234*i)\n",
    "\n",
    "                if dataset_size > 10000:\n",
    "                    train_epochs = 30\n",
    "                    top_percent = 20\n",
    "\n",
    "                elif dataset_size < 10000:\n",
    "                    train_epochs = 60\n",
    "                    top_percent = 50\n",
    "\n",
    "                model = scSpecies(device, \n",
    "                                mdata, \n",
    "                                path, \n",
    "                                k_neigh = k_neigh,\n",
    "                                alignment = alignment,\n",
    "                                top_percent = top_percent,\n",
    "                                context_dataset_key = context_key, \n",
    "                                target_dataset_key = target_key, \n",
    "                                random_seed = 1234*i\n",
    "                                )\n",
    "\n",
    "                model.train_context(30, save_model=False)\n",
    "                model.eval_context()\n",
    "\n",
    "                model.train_target(dataset_size, save_model=False)\n",
    "                model.eval_target()   \n",
    "\n",
    "                model.save_params(save_key, save='both', name='')\n",
    "\n",
    "                model.eval_label_transfer([('cell_type_coarse', 'cell_type_coarse'), ('cell_type_fine', 'cell_type_fine')])\n",
    "                \n",
    "                #adata = ad.AnnData(np.concat([model.mdata[context_key].obsm['latent_mu'], model.mdata[target_key].obsm['latent_mu']]))\n",
    "                #obs = pd.concat([model.mdata[context_key].obs, model.mdata[target_key].obs])\n",
    "                #obs['species'] = np.array(['context'] * model.mdata[context_key].n_obs + ['target'] * model.mdata[target_key].n_obs)\n",
    "                #adata.obs = obs\n",
    "\n",
    "                #adata.uns['ind_nns_aligned_latent_space'] = model.mdata[target_key].obsm['ind_nns_aligned_latent_space']\n",
    "                #adata.uns['nlog_likeli_nns_aligned_latent_space'] = model.mdata[target_key].obsm['nlog_likeli_nns_aligned_latent_space']\n",
    "\n",
    "                #adata.write(path+'save_adata/'+save_key+'.h5ad')\n",
    "                model.mdata.write(save_path+'/'+save_key+'.h5mu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dict_list = [\n",
    "                {'k_neigh': 25, 'alignment': 'latent'}, \n",
    "              ]\n",
    "\n",
    "dataset_list = [\n",
    "                {'dataset': \"liver_human\", 'context_key': 'mouse', 'target_key': 'human', 'load_key': 'liver'},                 \n",
    "               ]\n",
    "\n",
    "for reduced_genes in [200, 400, 600, 800, 1000, 1200, 1400, 1600]:\n",
    "\n",
    "    for dataset_dict in dataset_list:\n",
    "        dataset = dataset_dict['dataset']\n",
    "        context_key = dataset_dict['context_key']\n",
    "        target_key = dataset_dict['target_key']\n",
    "        load_key = dataset_dict['load_key']\n",
    "\n",
    "        for params in h_dict_list:\n",
    "\n",
    "            k_neigh = params['k_neigh']\n",
    "            alignment = params['alignment'] \n",
    "            \n",
    "            for i in range(10):\n",
    "                save_key = load_key +'_'+ context_key +'_'+ target_key +'_'+ str(k_neigh) +'_'+ alignment +'_'+ str(i) +'_reduced_'+ str(reduced_genes)\n",
    "\n",
    "                mdata = mu.read_h5mu(path+f\"dataset/{load_key}.h5mu\")\n",
    "\n",
    "                v_ctx = mdata.mod[context_key].var['human_gene_names'].astype(str).to_numpy()\n",
    "                v_tgt = mdata.mod[target_key].var['human_gene_names'].astype(str).to_numpy()\n",
    "\n",
    "                shared_genes = np.intersect1d(v_ctx, v_tgt)\n",
    "\n",
    "                perm = np.random.permutation(len(shared_genes))\n",
    "                rem = np.asarray(shared_genes[perm[:reduced_genes]], dtype=str)\n",
    "\n",
    "                context_ind = np.setdiff1d(np.arange(mdata.mod[context_key].n_vars), np.array([np.where(np.isin(v_ctx, r))[0][0] for r in rem]))\n",
    "                target_ind = np.setdiff1d(np.arange(mdata.mod[target_key].n_vars), np.array([np.where(np.isin(v_tgt, r))[0][0] for r in rem]))\n",
    "\n",
    "                preprocess = create_mdata(mdata.mod[context_key][:, context_ind], \n",
    "                                        context_batch_key='batch', \n",
    "                                        context_cell_key='cell_type_fine', \n",
    "                                        context_dataset_name=context_key, \n",
    "                                        context_gene_naming_convention=target_key)\n",
    "\n",
    "                preprocess.setup_target_adata(mdata.mod[target_key][:, target_ind], \n",
    "                                            target_batch_key='batch', \n",
    "                                            target_cell_key='cell_type_fine', \n",
    "                                            target_dataset_name=target_key, \n",
    "                                            target_gene_naming_convention=target_key)\n",
    "\n",
    "                model = scSpecies(device, \n",
    "                                mdata, \n",
    "                                path, \n",
    "                                k_neigh = k_neigh,\n",
    "                                alignment = alignment,\n",
    "                                top_percent = top_percent,\n",
    "                                context_dataset_key = context_key, \n",
    "                                target_dataset_key = target_key, \n",
    "                                random_seed = 1234*i\n",
    "                                )\n",
    "\n",
    "                model.train_context(30, save_model=False)\n",
    "                model.eval_context()\n",
    "\n",
    "                model.train_target(dataset_size, save_model=False)\n",
    "                model.eval_target()   \n",
    "\n",
    "                model.save_params(save_key, save='both', name='')\n",
    "\n",
    "                model.eval_label_transfer([('cell_type_coarse', 'cell_type_coarse'), ('cell_type_fine', 'cell_type_fine')])\n",
    "                \n",
    "                #adata = ad.AnnData(np.concat([model.mdata[context_key].obsm['latent_mu'], model.mdata[target_key].obsm['latent_mu']]))\n",
    "                #obs = pd.concat([model.mdata[context_key].obs, model.mdata[target_key].obs])\n",
    "                #obs['species'] = np.array(['context'] * model.mdata[context_key].n_obs + ['target'] * model.mdata[target_key].n_obs)\n",
    "                #adata.obs = obs\n",
    "\n",
    "                #adata.uns['ind_nns_aligned_latent_space'] = model.mdata[target_key].obsm['ind_nns_aligned_latent_space']\n",
    "                #adata.uns['nlog_likeli_nns_aligned_latent_space'] = model.mdata[target_key].obsm['nlog_likeli_nns_aligned_latent_space']\n",
    "                #adata.uns['context_ind'] = context_ind\n",
    "                #adata.uns['target_ind'] = target_ind\n",
    "                #adata.write(path+'save_adata/'+save_key+'.h5ad')\n",
    "                model.mdata.write(save_path+'/'+save_key+'.h5mu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scSpecies_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
