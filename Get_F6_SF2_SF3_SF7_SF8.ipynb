{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import gseapy as gp\n",
    "import  re\n",
    "import math\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import muon as mu\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import ttest_ind, entropy\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from create_datasets.preprocessing import create_mdata, set_random_seed\n",
    "from models import scSpecies\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_random_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "path = os.path.abspath('').replace('\\\\', '/')+'/'\n",
    "data_path = path+'dataset/'\n",
    "save_path = os.path.abspath('').replace('\\\\', '/')+'/results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "dataset = \"liver\"\n",
    "context_key = 'mouse'\n",
    "target_key = 'mouseNafld'\n",
    "load_key = 'liver'    \n",
    "\n",
    "save_key = dataset+'_'+context_key+'_'+target_key+'_'\n",
    "\n",
    "i=0\n",
    "set_random_seed(1234)\n",
    "\n",
    "mdata = mu.read_h5mu(data_path+dataset+\".h5mu\")\n",
    "\n",
    "h_dict = {'k_neigh': 25,  'alignment': 'inter',  'top_percent': 20, 'latent_dim':10}\n",
    "\n",
    "scSpecies_model = scSpecies(device, \n",
    "                mdata, \n",
    "                path,\n",
    "                context_dataset_key = context_key, \n",
    "                target_dataset_key = target_key,       \n",
    "                random_seed = 1234,\n",
    "                use_lib_enc = True,\n",
    "                **h_dict\n",
    "                )\n",
    "\n",
    "scSpecies_model.train_context(40, save_model=False)\n",
    "scSpecies_model.eval_context()\n",
    "\n",
    "scSpecies_model.train_target(40, track_prototypes=False, save_model=False)\n",
    "scSpecies_model.eval_target()\n",
    "\n",
    "scSpecies_model.get_representation('context', save_libsize=True) \n",
    "scSpecies_model.get_representation('target', save_libsize=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Log-fold change analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logfold_change(model, context_cell_key, target_cell_key, eps=1e-6, lfc_delta=1, samples=50000, b_s=128, confidence_level=0.9):\n",
    "    model.mdata.mod[model.context_dataset_key].uns['lfc_delta'] = lfc_delta\n",
    "    model.context_decoder.eval()   \n",
    "    model.target_decoder.eval()    \n",
    "    model.context_encoder_inner.eval()   \n",
    "    model.target_encoder_inner.eval() \n",
    "    model.context_encoder_outer.eval()   \n",
    "    model.target_encoder_outer.eval() \n",
    "    model.context_lib_encoder.eval()   \n",
    "    model.target_lib_encoder.eval()         \n",
    "                        \n",
    "    target_ind = np.array(model.target_param_dict['homologous_genes'])\n",
    "    target_gene_names = model.mdata.mod[model.target_dataset_key].var_names.to_numpy()[target_ind]\n",
    "\n",
    "    context_cell_labels = model.mdata.mod[model.context_dataset_key].obs[context_cell_key].to_numpy()\n",
    "    context_cell_types = np.unique(context_cell_labels)\n",
    "    context_cell_index = {c : np.where(context_cell_labels == c)[0] for c in context_cell_types}\n",
    "\n",
    "    target_cell_labels = model.mdata.mod[model.target_dataset_key].obs[target_cell_key].to_numpy()\n",
    "    target_cell_types = np.unique(target_cell_labels)\n",
    "    target_cell_index = {c : np.where(target_cell_labels == c)[0] for c in target_cell_types}\n",
    "\n",
    "    context_batch_key = model.mdata.mod[model.context_dataset_key].uns['dataset_batch_key']\n",
    "    target_batch_key = model.mdata.mod[model.target_dataset_key].uns['dataset_batch_key']\n",
    "    \n",
    "    context_batch_labels = model.mdata.mod[model.context_dataset_key].obs[context_batch_key].to_numpy().reshape(-1, 1)\n",
    "    target_batch_labels = model.mdata.mod[model.target_dataset_key].obs[target_batch_key].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    context_enc = OneHotEncoder()\n",
    "    context_enc.fit(context_batch_labels)\n",
    "\n",
    "    target_enc = OneHotEncoder()\n",
    "    target_enc.fit(target_batch_labels)\n",
    "\n",
    "    context_batches = {c : model.mdata.mod[model.context_dataset_key][model.mdata.mod[model.context_dataset_key].obs[context_cell_key] == c].obs[context_batch_key].value_counts() > 3 for c in context_cell_types}\n",
    "    context_batches = {c : context_batches[c][context_batches[c]].index.to_numpy() for c in context_cell_types}\n",
    "    context_batches = {c : context_enc.transform(context_batches[c].reshape(-1, 1)).toarray().astype(np.float32)  for c in context_cell_types}\n",
    "    context_batches['unknown'] = context_enc.transform(np.unique(context_batch_labels).reshape(-1, 1)).toarray().astype(np.float32)\n",
    "\n",
    "    if target_cell_key == None:\n",
    "        joint_cell_types = context_cell_types\n",
    "\n",
    "    else:\n",
    "        target_cell_labels = model.mdata.mod[model.target_dataset_key].obs[target_cell_key].to_numpy()\n",
    "        target_cell_types = np.unique(target_cell_labels)\n",
    "        joint_cell_types = np.intersect1d(context_cell_types, target_cell_types, return_indices=True)[0]\n",
    "        target_batches = {c : model.mdata.mod[model.target_dataset_key][model.mdata.mod[model.target_dataset_key].obs[target_cell_key] == c].obs[target_batch_key].value_counts() > 3 for c in target_cell_types}\n",
    "        target_batches = {c : target_batches[c][target_batches[c]].index.to_numpy() for c in target_cell_types}\n",
    "        target_batches = {c : target_enc.transform(target_batches[c].reshape(-1, 1)).toarray().astype(np.float32)  for c in target_cell_types}\n",
    "        target_batches['unknown'] = target_enc.transform(np.unique(target_batch_labels).reshape(-1, 1)).toarray().astype(np.float32)\n",
    "\n",
    "    lfc_dict = {}\n",
    "    random_perm = np.random.permutation(len(target_gene_names))\n",
    "\n",
    "    for cell_type in joint_cell_types:\n",
    "\n",
    "        adata_context = model.mdata.mod[model.context_dataset_key][context_cell_index[cell_type]]\n",
    "        adata_target = model.mdata.mod[model.target_dataset_key][target_cell_index[cell_type]]\n",
    "        \n",
    "        filtered_data_ind, _ = model.filter_outliers(adata_context.obsm['l_mu'], confidence_level=confidence_level)\n",
    "        adata_context = adata_context[filtered_data_ind]\n",
    "\n",
    "        filtered_data_ind, _ = model.filter_outliers(adata_target.obsm['l_mu'], confidence_level=confidence_level)\n",
    "        adata_target = adata_target[filtered_data_ind]      \n",
    "\n",
    "        latent_target = adata_target.obsm['l_mu']\n",
    "        latent_context = adata_context.obsm['l_mu']\n",
    "        nn = NearestNeighbors(n_neighbors=25, metric='cosine', algorithm='auto')\n",
    "        nn.fit(latent_context)\n",
    "        distances, indices = nn.kneighbors(latent_target)\n",
    "        adata_target.obsm['cell_context_ind'] = indices\n",
    "\n",
    "        steps = np.ceil(adata_target.n_obs/b_s).astype(int)    \n",
    "        iterations = int(np.ceil(samples/adata_target.n_obs))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logfold_list_rho = []    \n",
    "            rho_mouse = []     \n",
    "            mu_mouse = []        \n",
    "            rho_human = []     \n",
    "            mu_human = []                    \n",
    "\n",
    "            logfold_list_rho_random = []    \n",
    "\n",
    "            for iter in range(iterations):\n",
    "                for step in range(steps):   \n",
    "                    batch_adata = adata_target[step*b_s:(step+1)*b_s]\n",
    "                    context_cell_type = batch_adata.obs[batch_adata.uns['dataset_cell_key']].to_numpy()\n",
    "                    target_cell_type = batch_adata.obs[batch_adata.uns['dataset_cell_key']].to_numpy() \n",
    "\n",
    "                    context_labels = np.concatenate([context_batches[c] for c in context_cell_type])\n",
    "                    target_labels = np.concatenate([target_batches[c] for c in target_cell_type])\n",
    "                \n",
    "                    context_labels = torch.from_numpy(context_labels).to(model.device)\n",
    "                    target_labels = torch.from_numpy(target_labels).to(model.device)            \n",
    "\n",
    "                    context_ind_batch = np.array([np.shape(context_batches[c])[0] for c in context_cell_type])\n",
    "                    target_ind_batch = np.array([np.shape(target_batches[c])[0] for c in target_cell_type])\n",
    "\n",
    "                    shape = np.shape(batch_adata.obsm['latent_sig'])\n",
    "\n",
    "                    z = np.float32(batch_adata.obsm['latent_mu'] + batch_adata.obsm['latent_sig'] * np.random.rand(shape[0], shape[1])) \n",
    "                    target_l = np.exp(np.float32(batch_adata.obsm['l_mu'] + batch_adata.obsm['l_sig'] * np.random.rand(shape[0], 1)))\n",
    "                    neigh_ind = batch_adata.obsm['cell_context_ind']\n",
    "                    \n",
    "                    context_l = np.exp(np.float32(adata_context.obsm['l_mu'][neigh_ind] + adata_context.obsm['l_sig'][neigh_ind] * np.random.rand(shape[0], 25, 1)))\n",
    "                    context_l = context_l.mean(axis=1)\n",
    "\n",
    "                    context_z = np.concatenate([np.tile(z[j], (i, 1)) for j, i in enumerate(context_ind_batch)])\n",
    "                    target_z = np.concatenate([np.tile(z[j], (i, 1)) for j, i in enumerate(target_ind_batch)])\n",
    "\n",
    "                    context_z = torch.from_numpy(context_z).to(model.device)\n",
    "                    target_z = torch.from_numpy(target_z).to(model.device)\n",
    "\n",
    "                    context_rho = model.context_decoder.decode_homologous(context_z, context_labels).cpu().numpy()\n",
    "                    context_rho = model.average_slices(context_rho, context_ind_batch) \n",
    "\n",
    "                    target_rho = model.target_decoder.decode_homologous(target_z, target_labels).cpu().numpy()\n",
    "                    target_rho = model.average_slices(target_rho, target_ind_batch)\n",
    "\n",
    "                    context_mu = context_rho * context_l\n",
    "                    target_mu = target_rho * target_l\n",
    "\n",
    "                    rho_mouse.append(context_rho)\n",
    "                    mu_mouse.append(context_mu)\n",
    "                    rho_human.append(target_rho)\n",
    "                    mu_human.append(target_mu)\n",
    "\n",
    "                    logfold_list_rho.append(np.log2(target_rho+eps) - np.log2(context_rho+eps))\n",
    "                    logfold_list_rho_random.append(np.log2(target_rho+eps) - np.log2(context_rho[:, random_perm]+eps))\n",
    "\n",
    "\n",
    "        lfc_dict[cell_type] = pd.DataFrame(0, index=target_gene_names, columns=[\n",
    "            'rho_median_context', 'mu_median_context', 'rho_median_target', 'mu_median_target', 'lfc', 'p', 'lfc_rand', 'p_rand'])\n",
    "\n",
    "        rho_mouse = np.concatenate(rho_mouse)\n",
    "        mu_mouse = np.concatenate(mu_mouse)\n",
    "        rho_human = np.concatenate(rho_human)\n",
    "        mu_human = np.concatenate(mu_human)\n",
    "\n",
    "        lfc_dict[cell_type]['rho_median_context'] = np.median(rho_mouse, axis=0)\n",
    "        lfc_dict[cell_type]['mu_median_context'] = np.median(mu_mouse, axis=0)\n",
    "        lfc_dict[cell_type]['rho_median_target'] = np.median(rho_human, axis=0)\n",
    "        lfc_dict[cell_type]['mu_median_target'] = np.median(mu_human, axis=0)        \n",
    "\n",
    "        logfold_list_rho = np.concatenate(logfold_list_rho)\n",
    "\n",
    "        lfc_dict[cell_type]['lfc'] = np.median(logfold_list_rho, axis=0)\n",
    "        lfc_dict[cell_type]['p'] = np.sum(np.where(np.abs(logfold_list_rho)>lfc_delta, 1, 0), axis=0) / np.shape(logfold_list_rho)[0]\n",
    "        logfold_list_rho_random = np.concatenate(logfold_list_rho_random)\n",
    "\n",
    "        lfc_dict[cell_type]['lfc_rand'] = np.median(logfold_list_rho_random, axis=0)\n",
    "        lfc_dict[cell_type]['p_rand']  = np.sum(np.where(np.abs(logfold_list_rho_random)>lfc_delta, 1, 0), axis=0) / np.shape(logfold_list_rho_random)[0]\n",
    "\n",
    "    return lfc_dict\n",
    "        \n",
    "lfc_dict = compute_logfold_change(scSpecies_model, 'cell_type_fine', 'cell_type_fine', eps=1e-6, lfc_delta=0.4, samples=50000, b_s=128, confidence_level=0.95)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfc_delta = 1\n",
    "prob_delta = 0.9\n",
    "\n",
    "cell_types = list(lfc_dict.keys())\n",
    "df_lfc = pd.DataFrame({ct: lfc_dict[ct]['lfc'] for ct in lfc_dict.keys()})\n",
    "df_prob = pd.DataFrame({ct: lfc_dict[ct]['p'] for ct in lfc_dict.keys()})\n",
    "\n",
    "df_lfc_random = pd.DataFrame({ct: lfc_dict[ct]['lfc_rand'] for ct in lfc_dict.keys()})\n",
    "df_prob_random = pd.DataFrame({ct: lfc_dict[ct]['p_rand'] for ct in lfc_dict.keys()})\n",
    "\n",
    "n_cell_types = len(cell_types)\n",
    "n_cols = 4  \n",
    "n_rows = int(np.ceil(n_cell_types / n_cols))\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows), squeeze=False)\n",
    "axs = axs.flatten()\n",
    "\n",
    "ge_one = []\n",
    "ge_prob = []\n",
    "up_reg = []\n",
    "down_reg = []\n",
    "\n",
    "ge_one_random = []\n",
    "ge_prob_random = []\n",
    "up_reg_random = []\n",
    "down_reg_random = []\n",
    "\n",
    "for i, cell in enumerate(cell_types):\n",
    "\n",
    "    greater_than_one = np.round(((df_lfc[cell].abs() > lfc_delta).mean()*100), 1)\n",
    "    greater_than_one_prob = np.round(((df_lfc[cell].abs() >lfc_delta) & (df_prob[cell].abs() > prob_delta)).mean()*100, 1)\n",
    "    up = np.round(((df_lfc[cell] > lfc_delta) & (df_prob[cell].abs() > prob_delta)).mean()*100, 1)\n",
    "    down = np.round(((df_lfc[cell] < -lfc_delta) & (df_prob[cell].abs() > prob_delta)).mean()*100, 1)\n",
    "    \n",
    "    ge_one.append(greater_than_one)\n",
    "    ge_prob.append(greater_than_one_prob)\n",
    "    up_reg.append(up)\n",
    "    down_reg.append(down)\n",
    "\n",
    "    ge_one_random.append(np.round(((df_lfc_random[cell].abs() > lfc_delta).mean()*100), 1))\n",
    "    ge_prob_random.append(np.round(((df_lfc_random[cell].abs() > lfc_delta) & (df_prob_random[cell].abs() > prob_delta)).mean()*100, 1))\n",
    "    up_reg_random.append(np.round(((df_lfc_random[cell] > lfc_delta) & (df_prob_random[cell].abs() > prob_delta)).mean()*100, 1))\n",
    "    down_reg_random.append(np.round(((df_lfc_random[cell] < -lfc_delta) & (df_prob_random[cell].abs() > prob_delta)).mean()*100, 1))\n",
    "    \n",
    "    ax = axs[i]\n",
    "\n",
    "    colors = []\n",
    "    for l, p in zip(df_lfc[cell], df_prob[cell]):\n",
    "        if abs(l) <= 1:\n",
    "            colors.append('grey')\n",
    "        else:\n",
    "            if l > 1:\n",
    "\n",
    "                colors.append('red' if p > prob_delta else 'lightcoral')\n",
    "            elif l < -1:\n",
    "\n",
    "                colors.append('blue' if p > prob_delta else 'lightblue')\n",
    "    \n",
    "    ax.scatter(df_lfc[cell], df_prob[cell], c=colors, s=12, edgecolor='k')\n",
    "    ax.set_xlabel('Log Fold Change')\n",
    "    ax.set_ylabel('Probability')\n",
    "\n",
    "    ax.set_title(f\"{cell} |LFC|>1, with p>0.9: {greater_than_one_prob}\", pad=10)\n",
    "\n",
    "    ax.axhline(prob_delta, color='black', linestyle='--', linewidth=1.2)\n",
    "    ax.axvline(-1, color='black', linestyle='--', linewidth=1.2)\n",
    "    ax.axvline(1, color='black', linestyle='--', linewidth=1.2)\n",
    "\n",
    "    up_subset = df_lfc[cell][(df_lfc[cell] > lfc_delta) & (df_prob[cell].abs() > prob_delta)]\n",
    "    down_subset = df_lfc[cell][(df_lfc[cell] < -lfc_delta) & (df_prob[cell].abs() > prob_delta)]\n",
    "    \n",
    "    top_up = up_subset.sort_values(ascending=False).head(5)\n",
    "    top_down = down_subset.sort_values(ascending=True).head(5)\n",
    "\n",
    "    up_text = \"Upregulated:\\n\" + \"\\n\".join([f\"{j+1}. {gene}\" for j, gene in enumerate(top_up.index)])\n",
    "    down_text = \"Downregulated:\\n\" + \"\\n\".join([f\"{j+1}. {gene}\" for j, gene in enumerate(top_down.index)])\n",
    "\n",
    "    ax.text(0.025, 0.54, up_text, transform=ax.transAxes, verticalalignment='top',\n",
    "            fontsize=10, bbox=dict(boxstyle=\"round\", alpha=0.3, facecolor=\"white\"))\n",
    "    ax.text(0.98, 0.54, down_text, transform=ax.transAxes, verticalalignment='top',\n",
    "            horizontalalignment='right', fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round\", alpha=0.3, facecolor=\"white\"))\n",
    "\n",
    "for j in range(i+1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "# Adjust layout to leave space for the suptitle\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.suptitle(f'Differential gene expression analysis of a {context_key} context dataset aligned with a {target_key} target dataset.\\n' \n",
    "             f'Median |LFC|>1: {np.round(np.mean(ge_one),1)}%, with p>0.9: {np.round(np.mean(ge_prob),1)}. Up regulated: {np.round(np.mean(up_reg),1)}%, down regulated: {np.round(np.mean(down_reg),1)}%.\\n'\n",
    "             f'Permuted data, median |LFC|>1: {np.round(np.mean(ge_one_random),1)}%, with p>0.9: {np.round(np.mean(ge_prob_random),1)}. Up regulated: {np.round(np.mean(up_reg_random),1)}%, down regulated: {np.round(np.mean(down_reg_random),1)}%.',\n",
    "             y=0.99, fontsize=16)\n",
    "\n",
    "plt.savefig(save_path+save_key+\"DGE.pdf\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Correlation of LFC values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logfold_change(model, context_cell_key, target_cell_key, eps=1e-6, lfc_delta=1, samples=50000, b_s=128, confidence_level=0.9):\n",
    "    model.mdata.mod[model.context_dataset_key].uns['lfc_delta'] = lfc_delta\n",
    "    model.context_decoder.eval()   \n",
    "    model.target_decoder.eval()    \n",
    "    model.context_encoder_inner.eval()   \n",
    "    model.target_encoder_inner.eval() \n",
    "    model.context_encoder_outer.eval()   \n",
    "    model.target_encoder_outer.eval() \n",
    "    model.context_lib_encoder.eval()   \n",
    "    model.target_lib_encoder.eval()         \n",
    "                        \n",
    "    target_ind = np.array(model.target_param_dict['homologous_genes'])\n",
    "    target_gene_names = model.mdata.mod[model.target_dataset_key].var_names.to_numpy()[target_ind]\n",
    "\n",
    "    context_cell_labels = model.mdata.mod[model.context_dataset_key].obs[context_cell_key].to_numpy()\n",
    "    context_cell_types = np.unique(context_cell_labels)\n",
    "    context_cell_index = {c : np.where(context_cell_labels == c)[0] for c in context_cell_types}\n",
    "\n",
    "    target_cell_labels = model.mdata.mod[model.target_dataset_key].obs[target_cell_key].to_numpy()\n",
    "    target_cell_types = np.unique(target_cell_labels)\n",
    "    target_cell_index = {c : np.where(target_cell_labels == c)[0] for c in target_cell_types}\n",
    "\n",
    "    context_batch_key = model.mdata.mod[model.context_dataset_key].uns['dataset_batch_key']\n",
    "    target_batch_key = model.mdata.mod[model.target_dataset_key].uns['dataset_batch_key']\n",
    "    \n",
    "    context_batch_labels = model.mdata.mod[model.context_dataset_key].obs[context_batch_key].to_numpy().reshape(-1, 1)\n",
    "    target_batch_labels = model.mdata.mod[model.target_dataset_key].obs[target_batch_key].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    context_enc = OneHotEncoder()\n",
    "    context_enc.fit(context_batch_labels)\n",
    "\n",
    "    target_enc = OneHotEncoder()\n",
    "    target_enc.fit(target_batch_labels)\n",
    "\n",
    "    context_batches = {c : model.mdata.mod[model.context_dataset_key][model.mdata.mod[model.context_dataset_key].obs[context_cell_key] == c].obs[context_batch_key].value_counts() > 3 for c in context_cell_types}\n",
    "    context_batches = {c : context_batches[c][context_batches[c]].index.to_numpy() for c in context_cell_types}\n",
    "    context_batches = {c : context_enc.transform(context_batches[c].reshape(-1, 1)).toarray().astype(np.float32)  for c in context_cell_types}\n",
    "    context_batches['unknown'] = context_enc.transform(np.unique(context_batch_labels).reshape(-1, 1)).toarray().astype(np.float32)\n",
    "\n",
    "    if target_cell_key == None:\n",
    "        joint_cell_types = context_cell_types\n",
    "\n",
    "    else:\n",
    "        target_cell_labels = model.mdata.mod[model.target_dataset_key].obs[target_cell_key].to_numpy()\n",
    "        target_cell_types = np.unique(target_cell_labels)\n",
    "        joint_cell_types = np.intersect1d(context_cell_types, target_cell_types, return_indices=True)[0]\n",
    "        target_batches = {c : model.mdata.mod[model.target_dataset_key][model.mdata.mod[model.target_dataset_key].obs[target_cell_key] == c].obs[target_batch_key].value_counts() > 3 for c in target_cell_types}\n",
    "        target_batches = {c : target_batches[c][target_batches[c]].index.to_numpy() for c in target_cell_types}\n",
    "        target_batches = {c : target_enc.transform(target_batches[c].reshape(-1, 1)).toarray().astype(np.float32)  for c in target_cell_types}\n",
    "        target_batches['unknown'] = target_enc.transform(np.unique(target_batch_labels).reshape(-1, 1)).toarray().astype(np.float32)\n",
    "\n",
    "    df_lfc_rho = pd.DataFrame(0, index=target_gene_names, columns=joint_cell_types)\n",
    "    df_prob_rho = pd.DataFrame(0, index=target_gene_names, columns=joint_cell_types)\n",
    "\n",
    "    df_lfc_mu = pd.DataFrame(0, index=target_gene_names, columns=joint_cell_types)\n",
    "    df_prob_mu = pd.DataFrame(0, index=target_gene_names, columns=joint_cell_types)\n",
    "\n",
    "    df_lfc_rho_random = pd.DataFrame(0, index=target_gene_names, columns=joint_cell_types)\n",
    "    df_prob_rho_random = pd.DataFrame(0, index=target_gene_names, columns=joint_cell_types)\n",
    "\n",
    "    df_lfc_mu_random = pd.DataFrame(0, index=target_gene_names, columns=joint_cell_types)\n",
    "    df_prob_mu_random = pd.DataFrame(0, index=target_gene_names, columns=joint_cell_types)\n",
    "    \n",
    "    df_diff = pd.DataFrame(0, index=target_gene_names, columns=joint_cell_types)\n",
    "    \n",
    "    random_perm = np.random.permutation(len(target_gene_names))\n",
    "\n",
    "    for cell_type in joint_cell_types:\n",
    "\n",
    "        adata_context = model.mdata.mod[model.context_dataset_key][context_cell_index[cell_type]]\n",
    "        adata_target = model.mdata.mod[model.target_dataset_key][target_cell_index[cell_type]]\n",
    "        \n",
    "        filtered_data_ind, _ = model.filter_outliers(adata_context.obsm['latent_mu'], confidence_level=confidence_level)\n",
    "        adata_context = adata_context[filtered_data_ind]\n",
    "\n",
    "        filtered_data_ind, _ = model.filter_outliers(adata_target.obsm['latent_mu'], confidence_level=confidence_level)\n",
    "        adata_target = adata_target[filtered_data_ind]      \n",
    "\n",
    "        latent_target = adata_target.obsm['latent_mu']\n",
    "        latent_context = adata_context.obsm['latent_mu']\n",
    "        nn = NearestNeighbors(n_neighbors=25, metric='cosine', algorithm='auto')\n",
    "        nn.fit(latent_context)\n",
    "        distances, indices = nn.kneighbors(latent_target)\n",
    "        adata_target.obsm['cell_context_ind'] = indices\n",
    "\n",
    "        steps = np.ceil(adata_target.n_obs/b_s).astype(int)    \n",
    "        iterations = int(np.ceil(samples/adata_target.n_obs))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logfold_list_rho = []    \n",
    "            logfold_list_mu = []        \n",
    "            diff_list = []    \n",
    "\n",
    "            logfold_list_rho_random = []    \n",
    "            logfold_list_mu_random = []   \n",
    "\n",
    "            for iter in range(iterations):\n",
    "                for step in range(steps):   \n",
    "                    batch_adata = adata_target[step*b_s:(step+1)*b_s]\n",
    "                    context_cell_type = batch_adata.obs[batch_adata.uns['dataset_cell_key']].to_numpy()\n",
    "                    target_cell_type = batch_adata.obs[batch_adata.uns['dataset_cell_key']].to_numpy() #np.array(['unknown']*batch_adata.n_obs)\n",
    "\n",
    "                    context_labels = np.concatenate([context_batches[c] for c in context_cell_type])\n",
    "                    target_labels = np.concatenate([target_batches[c] for c in target_cell_type])\n",
    "                \n",
    "                    context_labels = torch.from_numpy(context_labels).to(model.device)\n",
    "                    target_labels = torch.from_numpy(target_labels).to(model.device)            \n",
    "\n",
    "                    context_ind_batch = np.array([np.shape(context_batches[c])[0] for c in context_cell_type])\n",
    "                    target_ind_batch = np.array([np.shape(target_batches[c])[0] for c in target_cell_type])\n",
    "\n",
    "                    shape = np.shape(batch_adata.obsm['latent_sig'])\n",
    "\n",
    "                    z = np.float32(batch_adata.obsm['latent_mu'] + batch_adata.obsm['latent_sig'] * np.random.rand(shape[0], shape[1])) \n",
    "                    target_l = np.exp(np.float32(batch_adata.obsm['l_mu'] + batch_adata.obsm['l_sig'] * np.random.rand(shape[0], 1)))\n",
    "                    neigh_ind = batch_adata.obsm['cell_context_ind']\n",
    "                    \n",
    "                    context_l = np.exp(np.float32(adata_context.obsm['l_mu'][neigh_ind] + adata_context.obsm['l_sig'][neigh_ind] * np.random.rand(shape[0], 25, 1)))\n",
    "                    context_l = context_l.mean(axis=1)\n",
    "\n",
    "                    context_z = np.concatenate([np.tile(z[j], (i, 1)) for j, i in enumerate(context_ind_batch)])\n",
    "                    target_z = np.concatenate([np.tile(z[j], (i, 1)) for j, i in enumerate(target_ind_batch)])\n",
    "\n",
    "                    context_z = torch.from_numpy(context_z).to(model.device)\n",
    "                    target_z = torch.from_numpy(target_z).to(model.device)\n",
    "\n",
    "                    context_rho = model.context_decoder.decode_homologous(context_z, context_labels).cpu().numpy()\n",
    "                    context_rho = model.average_slices(context_rho, context_ind_batch) \n",
    "\n",
    "                    target_rho = model.target_decoder.decode_homologous(target_z, target_labels).cpu().numpy()\n",
    "                    target_rho = model.average_slices(target_rho, target_ind_batch)\n",
    "\n",
    "                    context_mu = context_rho * context_l\n",
    "                    target_mu = target_rho * target_l\n",
    "\n",
    "                    logfold_list_rho.append(np.log2(target_rho+eps) - np.log2(context_rho+eps))\n",
    "                    logfold_list_mu.append(np.log2(target_mu+eps) - np.log2(context_mu+eps))\n",
    "\n",
    "                    logfold_list_rho_random.append(np.log2(target_rho+eps) - np.log2(context_rho[:, random_perm]+eps))\n",
    "                    logfold_list_mu_random.append(np.log2(target_mu+eps) - np.log2(context_mu[:, random_perm]+eps))\n",
    "\n",
    "                    diff_list.append(np.log2(context_l) - np.log2(target_l))\n",
    "\n",
    "        diff_list = np.mean(np.concatenate(diff_list))\n",
    "\n",
    "        logfold_list_rho = np.concatenate(logfold_list_rho)\n",
    "        logfold_list_mu = np.concatenate(logfold_list_mu)    \n",
    "\n",
    "        median_logfold_rho = np.median(logfold_list_rho, axis=0)\n",
    "        median_logfold_mu = np.median(logfold_list_mu, axis=0)\n",
    "\n",
    "        lfc_prob_rho = np.sum(np.where(np.abs(logfold_list_rho)>lfc_delta, 1, 0), axis=0) / np.shape(logfold_list_rho)[0]\n",
    "        lfc_prob_mu = np.sum(np.where(np.abs(logfold_list_mu)>lfc_delta, 1, 0), axis=0) / np.shape(logfold_list_mu)[0]\n",
    "\n",
    "        logfold_list_rho_random = np.concatenate(logfold_list_rho_random)\n",
    "        logfold_list_mu_random = np.concatenate(logfold_list_mu_random)    \n",
    "\n",
    "        median_logfold_rho_random = np.median(logfold_list_rho_random, axis=0)\n",
    "        median_logfold_mu_random = np.median(logfold_list_mu_random, axis=0)\n",
    "\n",
    "        lfc_prob_rho_random = np.sum(np.where(np.abs(logfold_list_rho_random)>lfc_delta, 1, 0), axis=0) / np.shape(logfold_list_rho_random)[0]\n",
    "        lfc_prob_mu_random = np.sum(np.where(np.abs(logfold_list_mu_random)>lfc_delta, 1, 0), axis=0) / np.shape(logfold_list_mu_random)[0]\n",
    "\n",
    "        df_lfc_rho[cell_type] = median_logfold_rho\n",
    "        df_prob_rho[cell_type] = lfc_prob_rho\n",
    "\n",
    "        df_lfc_mu[cell_type] = median_logfold_mu\n",
    "        df_prob_mu[cell_type] = lfc_prob_mu\n",
    "\n",
    "        df_lfc_rho_random[cell_type] = median_logfold_rho_random\n",
    "        df_prob_rho_random[cell_type] = lfc_prob_rho_random\n",
    "\n",
    "        df_lfc_mu_random[cell_type] = median_logfold_mu_random\n",
    "        df_prob_mu_random[cell_type] = lfc_prob_mu_random\n",
    "        \n",
    "        df_diff[cell_type] = diff_list\n",
    "\n",
    "    return df_lfc_rho, df_prob_rho, df_lfc_mu, df_prob_mu, df_lfc_rho_random, df_prob_rho_random, df_lfc_mu_random, df_prob_mu_random, df_diff\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_key == 'human':\n",
    "\n",
    "    df_lfc_rho, df_prob_rho, df_lfc_mu, df_prob_mu, df_lfc_rho_random, df_prob_rho_random, df_lfc_mu_random, df_prob_mu_random, df_diff = compute_logfold_change(scSpecies_model, 'cell_type_fine', 'cell_type_fine', eps=1e-6, lfc_delta=0.4, samples=1, b_s=128, confidence_level=0.95)         \n",
    "\n",
    "    target_ind = np.array(scSpecies_model.target_param_dict['homologous_genes'])\n",
    "    target_gene_names = scSpecies_model.mdata.mod[scSpecies_model.target_dataset_key].var_names.to_numpy()[target_ind]\n",
    "    joint_cell_types = list(lfc_dict.keys())\n",
    "\n",
    "    df_lfc_dat = pd.DataFrame(0, index=target_gene_names, columns=joint_cell_types)\n",
    "\n",
    "    spear = {}\n",
    "    pear = {}\n",
    "    kend = {}\n",
    "\n",
    "    for cell_type in joint_cell_types:\n",
    "        adata_target = scSpecies_model.mdata.mod['human'][:, np.array(scSpecies_model.target_param_dict['homologous_genes'])]\n",
    "        adata_context = scSpecies_model.mdata.mod['mouse'][:, np.array(scSpecies_model.context_param_dict['homologous_genes'])]\n",
    "\n",
    "        adata_target = adata_target[adata_target.obs['cell_type_fine'] == cell_type]\n",
    "        adata_context = adata_context[adata_context.obs['cell_type_fine'] == cell_type]    \n",
    "\n",
    "        sc.pp.normalize_total(adata_context, target_sum=1e6, inplace=True)\n",
    "        sc.pp.normalize_total(adata_target, target_sum=1e6, inplace=True)\n",
    "\n",
    "        adata_context = np.mean(adata_context.X.toarray(), axis=0)\n",
    "        adata_target = np.mean(adata_target.X.toarray(), axis=0)\n",
    "\n",
    "        lfc = np.log2(adata_target+1) - np.log2(adata_context+1) \n",
    "        df_lfc_dat[cell_type] = lfc\n",
    "\n",
    "        sort_data = np.argsort(lfc)\n",
    "        sort_model = np.argsort(lfc_dict[cell_type]['lfc'])\n",
    "\n",
    "        spear[cell_type] = spearmanr(lfc, lfc_dict[cell_type]['lfc']).statistic\n",
    "        pear[cell_type] = pearsonr(lfc, lfc_dict[cell_type]['lfc']).statistic\n",
    "        kend[cell_type] = kendalltau(np.arange(len(lfc)), np.argsort(lfc[sort_model])).statistic   \n",
    "\n",
    "    lfc_delta = 1\n",
    "    prob_delta = 0.9\n",
    "\n",
    "    cell_types = df_prob_rho.columns\n",
    "\n",
    "    n_cell_types = len(cell_types)\n",
    "    n_cols = 4  \n",
    "    n_rows = int(np.ceil(n_cell_types / n_cols))\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows), squeeze=False)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "\n",
    "    for i, cell_type in enumerate(cell_types):\n",
    "        ax = axs[i]\n",
    "\n",
    "        ax.scatter(np.array(df_lfc_dat[cell_type][np.argsort(df_lfc_rho[cell_type])]), df_lfc_rho[cell_type][np.argsort(df_lfc_rho[cell_type])], c='darkgrey', s=12, edgecolor='k')\n",
    "        ax.set_xlabel('LFC data-level')\n",
    "        ax.set_ylabel('LFC scSpecies')\n",
    "        ax.set_title(f\"{cell_type}\\n ρ: {str(round(spear[cell_type],2))}, PCC: {str(np.round(pear[cell_type],2))}, Kendall's τ: {str(round(kend[cell_type],2))}\", pad=10, fontsize=14)\n",
    "\n",
    "        lims = [\n",
    "            np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "            np.max([ax.get_xlim(), ax.get_ylim()])\n",
    "        ]\n",
    "\n",
    "        ax.plot(lims, lims, '--', color='red', linewidth=1) \n",
    "\n",
    "    for j in range(i+1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.suptitle(f\"LFC values derived by scSpecies vs. LFC values by a data-level analysis. \\n\" \n",
    "                f\"Average Spearman's ρ: {str(np.round(np.mean([spear[ct] for ct in spear.keys()]), 2))}, Pearson correlation: {str(np.round(np.mean([pear[ct] for ct in spear.keys()]), 2))}. Kendall's τ: {str(np.round(np.mean([kend[ct] for ct in spear.keys()]), 2))}.\", fontsize=18)\n",
    "\n",
    "    plt.savefig(save_path+save_key+\"Comparison_data_vs_model.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data-level reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, eval_key, b_s=128):\n",
    "\n",
    "    if eval_key == 'target':\n",
    "        dataset_key = model.target_dataset_key\n",
    "        decoder = model.target_decoder\n",
    "        encoder_inner =  model.target_encoder_inner\n",
    "        lib_encoder = model.target_lib_encoder\n",
    "        encoder_outer = model.target_encoder_outer  \n",
    "\n",
    "    if eval_key == 'context':\n",
    "        dataset_key = model.context_dataset_key\n",
    "        decoder = model.context_decoder\n",
    "        encoder_inner =  model.target_encoder_inner\n",
    "        lib_encoder = model.context_lib_encoder\n",
    "        encoder_outer = model.context_encoder_outer      \n",
    "        \n",
    "        \n",
    "    n_obs = model.mdata.mod[dataset_key].n_obs\n",
    "\n",
    "    steps_per_epoch = int(np.ceil(n_obs / b_s))\n",
    "\n",
    "    data_counts = torch.from_numpy(model.mdata.mod[dataset_key].X.toarray())\n",
    "\n",
    "    encoder_outer.eval()\n",
    "    lib_encoder.eval()        \n",
    "    decoder.eval()\n",
    "    encoder_inner.eval()\n",
    "\n",
    "    encoder_outer.cpu()\n",
    "    lib_encoder.cpu()        \n",
    "    decoder.cpu()\n",
    "    encoder_inner.cpu()\n",
    "\n",
    "    interm = {i: [] for i in range(len(encoder_outer.model) + len(encoder_inner.model))}\n",
    "    interm['mu'] = []\n",
    "\n",
    "    likeli_list = []\n",
    "    rec_list = []\n",
    "    norm_list = []\n",
    "    l_list = []\n",
    "    l_mu_list = []\n",
    "    l_log_sig_list = []\n",
    "\n",
    "    def process_layer(layer, input_tensor):\n",
    "        return layer(input_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(steps_per_epoch):\n",
    "            #print(step) \n",
    "            start_idx = step * b_s\n",
    "            end_idx = min((step + 1) * b_s, data_counts.size(0))\n",
    "            batch_adata = model.mdata.mod[dataset_key][start_idx:end_idx]\n",
    "\n",
    "            data_batch = data_counts[start_idx:end_idx]\n",
    "            label_batch = torch.from_numpy(batch_adata.obsm['batch_label_enc'])\n",
    "\n",
    "            x = torch.cat((data_batch, label_batch), dim=-1)  \n",
    "            for i, layer in enumerate(encoder_outer.model):\n",
    "                x = process_layer(layer, x)\n",
    "                interm[i].append(x) \n",
    "\n",
    "            for j, layer in enumerate(encoder_inner.model):\n",
    "                x = process_layer(layer, x)\n",
    "                interm[len(encoder_outer.model)+j].append(x) \n",
    "\n",
    "            mu = encoder_inner.mu(x)\n",
    "            interm['mu'].append(mu) \n",
    "            \n",
    "            inter = encoder_outer(data_batch, label_batch)\n",
    "            mu, log_sig = encoder_inner.encode(inter)                 \n",
    "            z = mu + log_sig.exp() * torch.rand_like(log_sig)\n",
    "            l_mu, l_log_sig = lib_encoder.encode(data_batch, label_batch)   \n",
    "            l = torch.exp(l_mu + l_log_sig.exp() * torch.rand_like(l_log_sig))\n",
    "\n",
    "            alpha, rho, pi_nlogit = decoder.decode(z, label_batch)   \n",
    "            eps = 1e-7        \n",
    "            alpha = torch.clamp(alpha, min=eps)\n",
    "            rho = torch.clamp(rho, min=eps, max=1 - eps)\n",
    "            pi = torch.sigmoid(-pi_nlogit)  \n",
    "            mu = rho * l \n",
    "\n",
    "            total_count = alpha\n",
    "            probs = torch.clamp(mu / (mu + alpha), min=eps, max=1 - eps)\n",
    "\n",
    "            zero_mask = torch.bernoulli(pi.expand(1, *pi.shape))\n",
    "            nb_dist = torch.distributions.NegativeBinomial(total_count=total_count, probs=probs)\n",
    "            samples = nb_dist.sample((1,)) \n",
    "            samples = torch.where(zero_mask.bool(), torch.zeros_like(samples), samples).squeeze()\n",
    "\n",
    "            log_alpha_mu = torch.log(alpha + mu)\n",
    "            log_likelihood = torch.where(data_batch < eps,\n",
    "                F.softplus(pi_nlogit + alpha * (torch.log(alpha) - log_alpha_mu)) - F.softplus(pi_nlogit),\n",
    "                - F.softplus(pi_nlogit) + pi_nlogit \n",
    "                + alpha * (torch.log(alpha) - log_alpha_mu) + data_batch * (torch.log(mu) - log_alpha_mu) \n",
    "                + torch.lgamma(data_batch + alpha) - torch.lgamma(alpha) - torch.lgamma(1.0 + data_batch))\n",
    "\n",
    "            likeli_list.append(log_likelihood.sum(-1))        \n",
    "            rec_list.append(samples.squeeze())\n",
    "            norm_list.append(rho.squeeze())\n",
    "            l_list.append(l)\n",
    "            l_mu_list.append(l_mu)\n",
    "            l_log_sig_list.append(l_log_sig)\n",
    "            \n",
    "        likeli_list = torch.concatenate(likeli_list)  \n",
    "        rec_list = torch.concatenate(rec_list)\n",
    "        norm_list = torch.concatenate(norm_list)\n",
    "        l_list = torch.concatenate(l_list)\n",
    "        l_mu_list = torch.concatenate(l_mu_list)\n",
    "        l_log_sig_list = torch.concatenate(l_log_sig_list)\n",
    "        \n",
    "    interm['data'] = sc.AnnData(model.mdata.mod[dataset_key].X)\n",
    "        \n",
    "    return interm, likeli_list, rec_list, norm_list, l_list, l_mu_list, l_log_sig_list, data_counts\n",
    "\n",
    "scvi_model = scSpecies(device, \n",
    "                mdata, \n",
    "                path,\n",
    "                context_dataset_key = target_key, \n",
    "                target_dataset_key = context_key,\n",
    "                train_only_scvi=True,          \n",
    "                random_seed = i*1234,\n",
    "                use_lib_enc = True,                \n",
    "                **h_dict\n",
    "                )\n",
    "\n",
    "\n",
    "scvi_model.train_context(30, save_model=False)\n",
    "scvi_model.eval_context()\n",
    "\n",
    "scSpecies_interm_context, scSpecies_likeli_list_context, scSpecies_rec_list_context, scSpecies_norm_list_context, scSpecies_l_list_context, scSpecies_l_mu_list_context, scSpecies_l_logsig_list_context, _ = eval_model(scSpecies_model, 'context', b_s=128)\n",
    "scSpecies_interm_target, scSpecies_likeli_list_target, scSpecies_rec_list_target, scSpecies_norm_list_target, scSpecies_l_list_target, scSpecies_l_mu_list_target, scSpecies_l_logsig_list_target, _ = eval_model(scSpecies_model, 'target', b_s=128)\n",
    "scvi_interm_target, scvi_likeli_list_target, scvi_rec_list_target, scvi_norm_list_target, scvi_l_list_target, scvi_l_mu_list_target, scvi_l_logsig_list_target, _ = eval_model(scvi_model, 'context', b_s=128)\n",
    "\n",
    "scSpecies_model.mdata.mod[context_key].obsm['l_latent_mu'] = np.array(scSpecies_l_mu_list_context)\n",
    "scSpecies_model.mdata.mod[context_key].obsm['l_latent_sig'] = np.array(scSpecies_l_logsig_list_context)\n",
    "scSpecies_model.mdata.mod[target_key].obsm['l_latent_mu'] = np.array(scSpecies_l_mu_list_target)\n",
    "scSpecies_model.mdata.mod[target_key].obsm['l_latent_sig'] = np.array(scSpecies_l_logsig_list_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_counts_human = torch.from_numpy(mdata.mod[target_key].X.toarray())\n",
    "data_counts_mouse = torch.from_numpy(mdata.mod[context_key].X.toarray())\n",
    "\n",
    "rho_pre_hom = (data_counts_human+1e-6)/(\n",
    "    data_counts_human[:, scSpecies_model.target_decoder.homologous_genes].sum(1).unsqueeze(-1) +1e-6*len(scSpecies_model.target_decoder.homologous_genes))\n",
    "rho_pre_nonhom = (data_counts_human[:, scSpecies_model.target_decoder.non_hom_genes]+1e-6)/(\n",
    "    data_counts_human[:, scSpecies_model.target_decoder.non_hom_genes].sum(1).unsqueeze(-1) +1e-6*len(scSpecies_model.target_decoder.non_hom_genes))\n",
    "\n",
    "data_counts_norm = torch.cat((rho_pre_hom, rho_pre_nonhom), dim=-1)[:, scSpecies_model.target_decoder.gene_ind]\n",
    "\n",
    "mdata.mod[target_key].layers['log1p'] = mdata.mod[target_key].X.copy()\n",
    "sc.pp.log1p(mdata.mod[target_key], layer='log1p')\n",
    "sc.tl.rank_genes_groups(mdata.mod[target_key], groupby='cell_type_fine', method='wilcoxon', layer='log1p')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import convolve1d\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "TITLE_SIZE = 17.5\n",
    "AXIS_LABEL_SIZE = 16.5\n",
    "TICK_LABEL_SIZE = 14\n",
    "LEGEND_SIZE = 17.5\n",
    "\n",
    "\n",
    "def get_top_marker_genes(cell_type, n_top=100):\n",
    "    groups = mdata.mod[target_key].uns['rank_genes_groups']['names'].dtype.names\n",
    "    top_genes = mdata.mod[target_key].uns['rank_genes_groups']['names'][:n_top][cell_type]\n",
    "    return list(top_genes)\n",
    "\n",
    "def histogram(data, num_bins=80, max_val=100):\n",
    "    \"\"\"Compute a smoothed density histogram using a Gaussian kernel.\"\"\"\n",
    "    data = np.asarray(data)\n",
    "    if data.size == 0:\n",
    "        return np.array([0]), np.array([0])\n",
    "    kernel_size = 7\n",
    "    sigma = 2.0\n",
    "    xk = np.linspace(- (kernel_size // 2), kernel_size // 2, kernel_size)\n",
    "    gauss_kernel = np.exp(-0.5 * (xk/sigma)**2)\n",
    "    gauss_kernel /= gauss_kernel.sum()\n",
    "    bin_edges = np.linspace(0, max_val, num_bins+1)\n",
    "    hist_counts, _ = np.histogram(data, bins=bin_edges, density=True)\n",
    "\n",
    "    first_bin_value = hist_counts[0]\n",
    "    positive_bins = hist_counts[1:]\n",
    "    positive_bins_smooth = convolve1d(positive_bins, gauss_kernel, mode='reflect')\n",
    "    final_hist = np.concatenate(([first_bin_value], positive_bins_smooth))\n",
    "    bin_mids = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    return bin_mids, np.clip(final_hist, a_min=0, a_max=0.4)\n",
    "\n",
    "\n",
    "cell_types_all = np.intersect1d(np.array(mdata.mod[target_key].obs.cell_type_fine.unique()),\n",
    "                                np.array(mdata.mod[context_key].obs.cell_type_fine.unique()))\n",
    "filtered = []\n",
    "for ct in cell_types_all:\n",
    "    ind = np.where(mdata.mod[target_key].obs.cell_type_fine == ct)[0]\n",
    "    if len(ind) >= 250:\n",
    "        filtered.append(ct)\n",
    "cell_types = np.array(filtered)\n",
    "n_cell_types = len(cell_types)\n",
    "n_top_genes = 3  \n",
    "\n",
    "fig, axes = plt.subplots(n_cell_types, 2 * n_top_genes,\n",
    "                          figsize=(3.3 * 2 * n_top_genes, 1.75 * n_cell_types),\n",
    "                          squeeze=False)\n",
    "\n",
    "for ct_idx, ct in enumerate(cell_types):\n",
    "\n",
    "    top_genes_human = get_top_marker_genes(ct, n_top=n_top_genes * 5)\n",
    "    top_genes_human = [gene for gene in top_genes_human \n",
    "                       if np.array(mdata.mod[target_key][:, mdata.mod[target_key].var_names == gene].var.mouse_gene_names)[0]\n",
    "                          in np.array(mdata.mod[context_key].var_names)]\n",
    "\n",
    "    top_genes_human = np.array(mdata.mod[target_key][:, np.isin(mdata.mod[target_key].var_names, top_genes_human)]\n",
    "                               .var_names)[:n_top_genes]\n",
    "    top_genes_mouse = np.array(mdata.mod[target_key][:, np.isin(mdata.mod[target_key].var_names, top_genes_human)]\n",
    "                               .var.mouse_gene_names)[:n_top_genes]\n",
    "\n",
    "    ind_h = np.where(mdata.mod[target_key].obs.cell_type_fine == ct)[0]\n",
    "    ind_m = np.where(mdata.mod[context_key].obs.cell_type_fine == ct)[0]\n",
    "    gene_ind = np.array([np.where(mdata.mod[target_key].var_names == gene)[0][0] \n",
    "                         for gene in top_genes_human])\n",
    "    gene_ind_m = np.array([np.where(mdata.mod[context_key].var_names == gene)[0][0] \n",
    "                           for gene in top_genes_mouse])\n",
    "\n",
    "    origs = data_counts_human[ind_h][:, gene_ind]\n",
    "    origs_mouse = data_counts_mouse[ind_m][:, gene_ind_m]\n",
    "    recs_scvi = scvi_norm_list_target[ind_h][:, gene_ind] * scvi_l_list_target[ind_h]\n",
    "    recs_scpecies = scSpecies_norm_list_target[ind_h][:, gene_ind] * scSpecies_l_list_target[ind_h]\n",
    "    recs_scpecies_m = scSpecies_norm_list_context[ind_m][:, gene_ind_m] * scSpecies_l_list_context[ind_m]\n",
    "\n",
    "    origs_norm = data_counts_norm[ind_h][:, gene_ind]\n",
    "    recs_scvi_norm = scvi_norm_list_target[ind_h][:, gene_ind]\n",
    "    recs_scpecies_norm = scSpecies_norm_list_target[ind_h][:, gene_ind]\n",
    "    \n",
    "    for j in range(n_top_genes):\n",
    "\n",
    "        d_obs   = np.array(origs[:, j])\n",
    "        d_obs_m = np.array(origs_mouse[:, j])\n",
    "        d_scvi  = np.array(recs_scvi[:, j])\n",
    "        d_scs   = np.array(recs_scpecies[:, j])\n",
    "        d_scs_m = np.array(recs_scpecies_m[:, j])\n",
    "\n",
    "        candidates = []\n",
    "        if d_obs.size > 0:\n",
    "            candidates.append(np.sort(d_obs)[int(len(d_obs)*0.96)])\n",
    "        if d_scvi.size > 0:\n",
    "            candidates.append(np.sort(d_scvi)[int(len(d_scvi)*0.96)])\n",
    "        if d_scs.size > 0:\n",
    "            candidates.append(np.sort(d_scs)[int(len(d_scs)*0.96)])\n",
    "\n",
    "        max_val_data = np.min(candidates + [1000])\n",
    "        max_val_data = np.max([max_val_data, 15, np.sort(d_scs_m)[int(len(d_scs_m)*0.9)]])\n",
    "        if ct == 'Hepatocytes':\n",
    "            max_val_data = 60\n",
    "        num_bins_data = int(min(max_val_data, 80))\n",
    "        \n",
    "        x_obs, y_obs         = histogram(d_obs, num_bins=num_bins_data, max_val=max_val_data)\n",
    "        x_obs_m, y_obs_m     = histogram(d_obs_m, num_bins=num_bins_data, max_val=max_val_data)\n",
    "        x_scvi, y_scvi       = histogram(d_scvi, num_bins=num_bins_data, max_val=max_val_data)\n",
    "        x_scs, y_scs         = histogram(d_scs, num_bins=num_bins_data, max_val=max_val_data)\n",
    "\n",
    "        ax_data = axes[ct_idx, 2*j]\n",
    "        ax_data.step(x_obs_m, y_obs_m, where='mid', color='orange', lw=2)\n",
    "        ax_data.step(x_obs, y_obs, where='mid', color='purple', lw=2)\n",
    "        ax_data.step(x_scvi, y_scvi, where='mid', color='darkgreen', lw=2)\n",
    "        ax_data.step(x_scs, y_scs, where='mid', color='blue', lw=2)\n",
    "        ax_data.set_title(f\"{top_genes_human[j]}/{top_genes_mouse[j]} (Raw)\", fontsize=TITLE_SIZE)\n",
    "        ax_data.tick_params(labelsize=TICK_LABEL_SIZE)\n",
    "\n",
    "        ax_data.set_xlabel('')\n",
    "\n",
    "        n_data  = np.array(origs_norm[:, j])\n",
    "        n_scvi  = np.array(recs_scvi_norm[:, j])\n",
    "        n_scs   = np.array(recs_scpecies_norm[:, j])\n",
    "        \n",
    "        max_val_norm = max(#np.max(n_data)*1.1 if n_data.size > 0 else 0,\n",
    "                           np.max(n_scvi)*1.1 if n_scvi.size > 0 else 0,\n",
    "                           np.max(n_scs)*1.1 if n_scs.size > 0 else 0,\n",
    "                           0.005)\n",
    "        bins_norm = np.linspace(0, max_val_norm, 100)\n",
    "        counts_data, _ = np.histogram(n_data, bins=bins_norm, density=True)\n",
    "        counts_scvi, _ = np.histogram(n_scvi, bins=bins_norm, density=True)\n",
    "        counts_scs, _  = np.histogram(n_scs, bins=bins_norm, density=True)\n",
    "        x_vals_norm = (bins_norm[:-1] + bins_norm[1:]) / 2\n",
    "\n",
    "        ax_norm = axes[ct_idx, 2*j+1]\n",
    "        ax_norm.step(x_vals_norm, counts_scvi, where='mid', color='deepskyblue', lw=2)\n",
    "        ax_norm.step(x_vals_norm, counts_scs, where='mid', color='limegreen', lw=2)\n",
    "        ax_norm.set_title(f\"{top_genes_human[j]} (Normalized)\", fontsize=TITLE_SIZE)\n",
    "        ax_norm.tick_params(labelsize=TICK_LABEL_SIZE)\n",
    "        ax_norm.set_xlabel('')\n",
    "        ax_norm.set_ylabel('')\n",
    "\n",
    "\n",
    "for ct_idx, ct in enumerate(cell_types):\n",
    "    \n",
    "    \n",
    "    if ct == 'Cholangiocytes':\n",
    "        axes[ct_idx, 0].set_ylabel(f\"{ct}\\n p.d.f\", fontsize=AXIS_LABEL_SIZE-2)\n",
    "    \n",
    "    elif ct == 'Cytotoxic CD8+':\n",
    "        axes[ct_idx, 0].set_ylabel(f\"{ct}\\n p.d.f\", fontsize=AXIS_LABEL_SIZE-2)\n",
    "        \n",
    "    else: \n",
    "        axes[ct_idx, 0].set_ylabel(f\"{ct}\\n p.d.f\", fontsize=AXIS_LABEL_SIZE)    \n",
    "    \n",
    "    if ct_idx == len(cell_types) - 1:\n",
    "        axes[ct_idx, 0].set_xlabel(f\"Gene expression levels\", fontsize=AXIS_LABEL_SIZE)\n",
    "        axes[ct_idx, 1].set_xlabel(f\"Normalized gene expression\", fontsize=AXIS_LABEL_SIZE)\n",
    "        axes[ct_idx, 2].set_xlabel(f\"Gene expression levels\", fontsize=AXIS_LABEL_SIZE)        \n",
    "        axes[ct_idx, 3].set_xlabel(f\"Normalized gene expression\", fontsize=AXIS_LABEL_SIZE)        \n",
    "        axes[ct_idx, 4].set_xlabel(f\"Gene expression levels\", fontsize=AXIS_LABEL_SIZE)\n",
    "        axes[ct_idx, 5].set_xlabel(f\"Normalized gene expression\", fontsize=AXIS_LABEL_SIZE)        \n",
    "\n",
    "norm_handles = [Line2D([0], [0], color='deepskyblue', lw=2, label=f'Normalized {target_key} scVI gene expression'),\n",
    "                Line2D([0], [0], color='limegreen', lw=2, label=f'Normalized {target_key} scSpecies gene expression'),\n",
    "                Line2D([0], [0], color='blue', lw=2, label=f'scVI {target_key} NB mean parameter'),\n",
    "                Line2D([0], [0], color='darkgreen', lw=2, label=f'scSpecies {target_key} NB mean parameter'),\n",
    "                Line2D([0], [0], color='purple', lw=2, label=f'{target_key} gene expression data distribution'.capitalize()),\n",
    "                Line2D([0], [0], color='orange', lw=2, label=f'{context_key} gene expression data distribution'.capitalize()),\n",
    "                ]\n",
    "fig.legend(norm_handles, [h.get_label() for h in norm_handles],\n",
    "           loc='upper center', bbox_to_anchor=(0.5, 0.98), ncol=3, fontsize=LEGEND_SIZE)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.04, 1, 0.95])\n",
    "plt.savefig(save_path+save_key+\"gene_expression_levels.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) ELBO convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_likeli = np.array(scvi_model.context_likeli_hist_dict) \n",
    "target_likeli = np.array(scSpecies_model.target_likeli_hist_dict) \n",
    "\n",
    "adaptive_smoothed_context = []\n",
    "adaptive_smoothed_target = []\n",
    "adaptive_indices = []\n",
    "\n",
    "for i in range(10, len(context_likeli)):\n",
    "    smooth_window = max(10, int(i*0.02))   \n",
    "    if i >= smooth_window:\n",
    "        adaptive_smoothed_context.append(np.mean(context_likeli[i - smooth_window:i]))\n",
    "        adaptive_smoothed_target.append(np.mean(target_likeli[i - smooth_window:i]))\n",
    "        adaptive_indices.append(i)\n",
    "\n",
    "adaptive_smoothed_context = np.array(adaptive_smoothed_context)\n",
    "adaptive_smoothed_target = np.array(adaptive_smoothed_target)\n",
    "adaptive_indices = np.array(adaptive_indices)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(adaptive_indices, adaptive_smoothed_context, label=\"scVI\", linestyle='-', color='blue')\n",
    "plt.plot(adaptive_indices, adaptive_smoothed_target, label=\"scSpecies\", linestyle='-', color='orange')\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks([10, 100, 1000, 10000], labels=[\"10\", \"100\", \"1000\", \"10000\"])\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Negative Log-likelihood\")\n",
    "plt.xlim(10,35000)\n",
    "plt.ylim(1100, 1700)\n",
    "plt.title(f\"Log-likelihood smoothed over the last 2% of iterations\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path+save_key+\"likelihood_during_training.pdf\")\n",
    "plt.show()\n",
    "\n",
    "print('scVI_final_ELBO', adaptive_smoothed_context[-1])\n",
    "print('scSpecies_final_ELBO', adaptive_smoothed_target[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Overrepresentation Enrichment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ora(ora_raw, N=10, q=None, cols=4, wrap=28, figsize_scale=4, title=\"Over-representation analysis, top 8 per cell type\", legend_ncol=2):\n",
    "    celltypes = list(ora_raw.keys())\n",
    "    rows = max(1, math.ceil(len(celltypes)/cols))\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(4*cols, figsize_scale*rows), squeeze=False)\n",
    "    eps = np.nextafter(0,1)\n",
    "    handles_labels = None\n",
    "    for i, ct in enumerate(celltypes):\n",
    "        ax = axes[i//cols, i%cols]\n",
    "        up = ora_raw[ct].get('up', pd.DataFrame(columns=['Term','Adjusted P-value']))[['Term','Adjusted P-value']].dropna().copy()\n",
    "        down = ora_raw[ct].get('down', pd.DataFrame(columns=['Term','Adjusted P-value']))[['Term','Adjusted P-value']].dropna().copy()\n",
    "        if up.empty and down.empty:\n",
    "            ax.axis('off'); continue\n",
    "        up['dir'] = 'up'; down['dir'] = 'down'\n",
    "        df = pd.concat([up, down], ignore_index=True)\n",
    "        if q is not None: df = df[df['Adjusted P-value']<=q]\n",
    "        if df.empty:\n",
    "            ax.axis('off'); continue\n",
    "        df['score'] = -np.log10(df['Adjusted P-value'].replace(0, eps))\n",
    "        df['label'] = [\"\\n\".join(textwrap.wrap(t, width=wrap)) for t in df['Term']]\n",
    "        df = df.nlargest(N, 'score')\n",
    "        sns.barplot(x='score', y='label', data=df, hue='dir', hue_order=['up','down'], dodge=False, orient='h', ax=ax)\n",
    "        if handles_labels is None:\n",
    "            handles_labels = ax.get_legend_handles_labels()\n",
    "        if ax.legend_ is not None:\n",
    "            ax.legend_.remove()\n",
    "        ax.set_title(ct, fontsize=12)\n",
    "        ax.set_xlabel('-log10(adj. p-val.)'); ax.set_ylabel('')\n",
    "        ax.tick_params(axis='y', which='both', labelsize=8)\n",
    "    for j in range(len(celltypes), rows*cols):\n",
    "        fig.delaxes(axes.flatten()[j])\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.82])\n",
    "    fig.suptitle(title, y=0.85, fontsize=14)\n",
    "    if handles_labels is not None:\n",
    "        handles, labels = handles_labels\n",
    "        fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.837), ncol=legend_ncol, frameon=True)  \n",
    "        plt.savefig(save_path+save_key+\"ORA.pdf\", bbox_inches='tight')  \n",
    "\n",
    "if target_key == 'human':\n",
    "\n",
    "    libs = set(gp.get_library_name(organism='Human'))\n",
    "    def pick(regex):\n",
    "        c = [x for x in libs if re.search(regex, x, re.I)]\n",
    "        c = sorted(c, key=lambda s: int(re.search(r'(\\d{4})', s).group(1)) if re.search(r'(\\d{4})', s) else 0)\n",
    "        return c[-1] if c else None\n",
    "\n",
    "    ORA_LIBS = [pick(r'^Reactome'), pick(r'^KEGG_.*Human'), pick(r'^GO_Biological_Process'), pick(r'Hallmark')]\n",
    "    ORA_LIBS = [x for x in ORA_LIBS if x]\n",
    "    print(ORA_LIBS)\n",
    "\n",
    "    tau = 0.95\n",
    "    lfc_thr = 1.0\n",
    "\n",
    "    universe = sorted(set().union(*[set(df.index) for df in lfc_dict.values()]))\n",
    "\n",
    "    deg = {}\n",
    "    for ct, df in lfc_dict.items():\n",
    "        up = df.index[(df['lfc']>=lfc_thr) & (df['p']>=tau)]\n",
    "        down = df.index[(df['lfc']<=-lfc_thr) & (df['p']>=tau)]\n",
    "        if len(up) >= 5 or len(down) >= 5:\n",
    "            deg[ct] = {'up': list(up), 'down': list(down)}\n",
    "\n",
    "    def run_enrichr(genes):\n",
    "        if len(genes) < 5:\n",
    "            return pd.DataFrame(columns=['Term','Adjusted P-value'])\n",
    "        r = gp.enrichr(gene_list=genes, gene_sets=ORA_LIBS, organism='human', background=universe, outdir=None, no_plot=True).results\n",
    "        return r[['Term','Adjusted P-value']].sort_values('Adjusted P-value').reset_index(drop=True)\n",
    "\n",
    "    ora_raw = {}\n",
    "    for ct in deg.keys():\n",
    "        ru = run_enrichr(deg[ct]['up'])\n",
    "        rd = run_enrichr(deg[ct]['down'])\n",
    "        ora_raw[ct] = {'up': ru, 'down': rd}\n",
    "\n",
    "    plot_ora(ora_raw, N=8)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Pathway Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_samples(model, context_cell_key, target_cell_key, samples=5000, b_s=128, confidence_level=0.9):\n",
    "    model.context_decoder.eval()   \n",
    "    model.target_decoder.eval()    \n",
    "    model.context_encoder_inner.eval()   \n",
    "    model.target_encoder_inner.eval() \n",
    "    model.context_encoder_outer.eval()   \n",
    "    model.target_encoder_outer.eval() \n",
    "    model.context_lib_encoder.eval()   \n",
    "    model.target_lib_encoder.eval()         \n",
    "\n",
    "    context_cell_labels = model.mdata.mod[model.context_dataset_key].obs[context_cell_key].to_numpy()\n",
    "    context_cell_types = np.unique(context_cell_labels)\n",
    "\n",
    "    target_cell_labels = model.mdata.mod[model.target_dataset_key].obs[target_cell_key].to_numpy()\n",
    "    target_cell_types = np.unique(target_cell_labels)\n",
    "    target_cell_index = {c : np.where(target_cell_labels == c)[0] for c in target_cell_types}\n",
    "\n",
    "    context_batch_key = model.mdata.mod[model.context_dataset_key].uns['dataset_batch_key']\n",
    "    target_batch_key = model.mdata.mod[model.target_dataset_key].uns['dataset_batch_key']\n",
    "    \n",
    "    context_batch_labels = model.mdata.mod[model.context_dataset_key].obs[context_batch_key].to_numpy().reshape(-1, 1)\n",
    "    target_batch_labels = model.mdata.mod[model.target_dataset_key].obs[target_batch_key].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    context_enc = OneHotEncoder()\n",
    "    context_enc.fit(context_batch_labels)\n",
    "\n",
    "    target_enc = OneHotEncoder()\n",
    "    target_enc.fit(target_batch_labels)\n",
    "\n",
    "    context_batches = {c : model.mdata.mod[model.context_dataset_key][model.mdata.mod[model.context_dataset_key].obs[context_cell_key] == c].obs[context_batch_key].value_counts() > 3 for c in context_cell_types}\n",
    "    context_batches = {c : context_batches[c][context_batches[c]].index.to_numpy() for c in context_cell_types}\n",
    "    context_batches = {c : context_enc.transform(context_batches[c].reshape(-1, 1)).toarray().astype(np.float32)  for c in context_cell_types}\n",
    "    context_batches['unknown'] = context_enc.transform(np.unique(context_batch_labels).reshape(-1, 1)).toarray().astype(np.float32)\n",
    "\n",
    "    if target_cell_key == None:\n",
    "        joint_cell_types = context_cell_types\n",
    "\n",
    "    else:\n",
    "        target_cell_labels = model.mdata.mod[model.target_dataset_key].obs[target_cell_key].to_numpy()\n",
    "        target_cell_types = np.unique(target_cell_labels)\n",
    "        joint_cell_types = np.intersect1d(context_cell_types, target_cell_types, return_indices=True)[0]\n",
    "        target_batches = {c : model.mdata.mod[model.target_dataset_key][model.mdata.mod[model.target_dataset_key].obs[target_cell_key] == c].obs[target_batch_key].value_counts() > 3 for c in target_cell_types}\n",
    "        target_batches = {c : target_batches[c][target_batches[c]].index.to_numpy() for c in target_cell_types}\n",
    "        target_batches = {c : target_enc.transform(target_batches[c].reshape(-1, 1)).toarray().astype(np.float32)  for c in target_cell_types}\n",
    "        target_batches['unknown'] = target_enc.transform(np.unique(target_batch_labels).reshape(-1, 1)).toarray().astype(np.float32)\n",
    "\n",
    "    context_rho_dict = {}\n",
    "    target_rho_dict = {}\n",
    "\n",
    "    for cell_type in joint_cell_types:\n",
    "        adata_target = model.mdata.mod[model.target_dataset_key][target_cell_index[cell_type]]\n",
    "\n",
    "        filtered_data_ind, _ = model.filter_outliers(adata_target.obsm['latent_mu'], confidence_level=confidence_level)\n",
    "        adata_target = adata_target[filtered_data_ind]      \n",
    "\n",
    "        steps = np.ceil(adata_target.n_obs/b_s).astype(int)    \n",
    "        iterations = int(np.ceil(samples/adata_target.n_obs))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            context_rho_dict[cell_type] = []    \n",
    "            target_rho_dict[cell_type] = []  \n",
    "\n",
    "            for iter in range(iterations):\n",
    "                for step in range(steps):   \n",
    "                    batch_adata = adata_target[step*b_s:(step+1)*b_s]\n",
    "                    context_cell_type = batch_adata.obs[batch_adata.uns['dataset_cell_key']].to_numpy()\n",
    "                    target_cell_type = batch_adata.obs[batch_adata.uns['dataset_cell_key']].to_numpy() #np.array(['unknown']*batch_adata.n_obs)\n",
    "\n",
    "                    context_labels = np.concatenate([context_batches[c] for c in context_cell_type])\n",
    "                    target_labels = np.concatenate([target_batches[c] for c in target_cell_type])\n",
    "                \n",
    "                    context_labels = torch.from_numpy(context_labels).to(model.device)\n",
    "                    target_labels = torch.from_numpy(target_labels).to(model.device)            \n",
    "\n",
    "                    context_ind_batch = np.array([np.shape(context_batches[c])[0] for c in context_cell_type])\n",
    "                    target_ind_batch = np.array([np.shape(target_batches[c])[0] for c in target_cell_type])\n",
    "\n",
    "                    shape = np.shape(batch_adata.obsm['z_sig'])\n",
    "                    z = np.float32(batch_adata.obsm['z_mu'] + batch_adata.obsm['z_sig'] * np.random.rand(shape[0], shape[1])) \n",
    "\n",
    "                    context_z = np.concatenate([np.tile(z[j], (i, 1)) for j, i in enumerate(context_ind_batch)])\n",
    "                    target_z = np.concatenate([np.tile(z[j], (i, 1)) for j, i in enumerate(target_ind_batch)])\n",
    "\n",
    "                    context_z = torch.from_numpy(context_z).to(model.device)\n",
    "                    target_z = torch.from_numpy(target_z).to(model.device)\n",
    "\n",
    "                    context_rho = model.context_decoder.decode_homologous(context_z, context_labels).cpu().numpy()\n",
    "                    context_rho = model.average_slices(context_rho, context_ind_batch)\n",
    "\n",
    "                    target_rho = model.target_decoder.decode_homologous(target_z, target_labels).cpu().numpy()\n",
    "                    target_rho = model.average_slices(target_rho, target_ind_batch)\n",
    "\n",
    "                    context_rho_dict[cell_type].append(context_rho)\n",
    "                    target_rho_dict[cell_type].append(target_rho)\n",
    "\n",
    "        target_rho_dict[cell_type] = np.concatenate(target_rho_dict[cell_type])[:samples]\n",
    "        context_rho_dict[cell_type] = np.concatenate(context_rho_dict[cell_type])[:samples]  \n",
    "\n",
    "    return target_rho_dict, context_rho_dict\n",
    "\n",
    "\n",
    "def load_and_filter_pathways(gmt_path, adata, min_genes=5):\n",
    "\n",
    "    pathways = {}\n",
    "    with open(gmt_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            name = parts[0]\n",
    "            gene_list = parts[2:]\n",
    "            pathways[name] = gene_list\n",
    "\n",
    "    # Filter\n",
    "    var_set = set(adata.var_names)\n",
    "    filtered = {}\n",
    "    for name, genes in pathways.items():\n",
    "        overlap = var_set.intersection(genes)\n",
    "        if len(overlap) >= min_genes:\n",
    "            filtered[name] = list(overlap)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def ensure_model_device(model):\n",
    "    device = torch.device(model.device if isinstance(model.device, str) else model.device)\n",
    "    for m in [\n",
    "        model.context_decoder, model.target_decoder,\n",
    "        model.context_encoder_inner, model.target_encoder_inner,\n",
    "        model.context_encoder_outer, model.target_encoder_outer,\n",
    "        model.context_lib_encoder, model.target_lib_encoder\n",
    "    ]:\n",
    "        m.to(device)\n",
    "    return device\n",
    "\n",
    "def mode_histogram(x, bins='fd'):         \n",
    "\n",
    "    counts, edges = np.histogram(x, bins=bins)\n",
    "    j = np.argmax(counts)                \n",
    "    return (edges[j] + edges[j+1]) / 2.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_key == 'human':\n",
    "\n",
    "    ensure_model_device(scSpecies_model)\n",
    "\n",
    "    target_rho_dict, context_rho_dict = decode_samples(scSpecies_model, 'cell_type_fine', 'cell_type_fine', samples=1000, b_s=128, confidence_level=0.95)\n",
    "\n",
    "    adata_h = ad.concat([ad.AnnData(target_rho_dict[key]) for key in target_rho_dict.keys()])\n",
    "    adata_h.var_names = scSpecies_model.mdata.mod['human'][:, scSpecies_model.target_param_dict['homologous_genes']].var_names\n",
    "    adata_h.obs['cell_type_fine'] = np.concat([[key]*np.shape(target_rho_dict[key])[0] for key in target_rho_dict.keys()])\n",
    "\n",
    "    adata_m = ad.concat([ad.AnnData(context_rho_dict[key]) for key in context_rho_dict.keys()])\n",
    "    adata_m.var_names = adata_h.var_names\n",
    "    adata_m.obs['cell_type_fine'] = np.concat([[key]*np.shape(context_rho_dict[key])[0] for key in context_rho_dict.keys()])\n",
    "\n",
    "    gene_sets_path = path+'dataset/c2.all.v2024.1.Hs.symbols.gmt'\n",
    "    pathways = load_and_filter_pathways(gene_sets_path, adata_h)\n",
    "\n",
    "    adata = adata_m.concatenate(\n",
    "        adata_h,\n",
    "        batch_key=\"species\",\n",
    "        batch_categories=[\"mouse\", \"human\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "    for i,key in enumerate(pathways.keys()):\n",
    "        sc.tl.score_genes(adata, gene_list=pathways[key], score_name=key)\n",
    "\n",
    "    scores_cols = list(adata.obs.columns[2:])\n",
    "\n",
    "    summary_scores = {ct: {} for ct in np.unique(adata.obs['cell_type_fine'].values)}\n",
    "\n",
    "    for ct in summary_scores.keys():\n",
    "        adata_cell_m = adata[adata.obs['cell_type_fine'] == ct]\n",
    "        adata_cell_m = adata_cell_m[adata_cell_m.obs['species'] == 'mouse']\n",
    "        adata_cell_h = adata[adata.obs['cell_type_fine'] == ct]\n",
    "        adata_cell_h = adata_cell_h[adata_cell_h.obs['species'] == 'human']\n",
    "        summary_scores[ct] = {pathway: (mode_histogram(adata_cell_m.obs[pathway]), mode_histogram(adata_cell_h.obs[pathway]), mode_histogram(adata_cell_m.obs[pathway]) - mode_histogram(adata_cell_h.obs[pathway])) for pathway in scores_cols}\n",
    "\n",
    "    mean_diff = {ct: np.mean([np.abs(summary_scores[ct][pathway][-1]) for pathway in scores_cols]) for ct in np.unique(adata.obs['cell_type_fine'].values)}\n",
    "    top_down = {ct: (scores_cols[np.argmin([summary_scores[ct][pathway][-1] for pathway in scores_cols])], np.min([summary_scores[ct][pathway][-1] for pathway in scores_cols])) for ct in np.unique(adata.obs['cell_type_fine'].values)}\n",
    "    top_up = {ct: (scores_cols[np.argmax([summary_scores[ct][pathway][-1] for pathway in scores_cols])], np.max([summary_scores[ct][pathway][-1] for pathway in scores_cols])) for ct in np.unique(adata.obs['cell_type_fine'].values)}\n",
    "    top_diff = {ct: (scores_cols[np.argmax([summary_scores[ct][pathway][-1] for pathway in scores_cols])], np.max([np.abs(summary_scores[ct][pathway][-1]) for pathway in scores_cols])) for ct in np.unique(adata.obs['cell_type_fine'].values)}\n",
    "\n",
    "    def wrap_name(name, width=30):\n",
    "        pretty = name.replace('_',' ')\n",
    "        return '\\n'.join(textwrap.wrap(pretty, width=width))\n",
    "\n",
    "    cell_types = list(top_down.keys())\n",
    "    n = len(cell_types)\n",
    "    cols = 5\n",
    "    rows = math.ceil(n/cols)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4), sharey=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, ct in zip(axes, cell_types):\n",
    "        pw_up   = top_up[ct][0]\n",
    "        pw_down = top_down[ct][0]\n",
    "        mean_val = mean_diff[ct]\n",
    "\n",
    "        df_ct = (\n",
    "            adata[adata.obs['cell_type_fine'] == ct].obs\n",
    "            .loc[:, ['species', pw_up, pw_down]]\n",
    "            .melt(id_vars='species',\n",
    "                value_vars=[pw_up, pw_down],\n",
    "                var_name='pathway',\n",
    "                value_name='score')\n",
    "        )\n",
    "\n",
    "        df_ct['pathway'] = df_ct['pathway'].map({\n",
    "            pw_up:   'Top-up',\n",
    "            pw_down: 'Top-down'\n",
    "        })\n",
    "\n",
    "        sns.violinplot(\n",
    "            ax=ax, data=df_ct,\n",
    "            x='pathway', y='score', hue='species',\n",
    "            palette={'human':'C0','mouse':'C1'},\n",
    "            dodge=True, inner='quartile'\n",
    "        )\n",
    "\n",
    "        title = (\n",
    "            f\"{ct}, Δ mean = {mean_val:.6f}\\n\"\n",
    "            f\"Up:   {wrap_name(pw_up)}\\n\"\n",
    "            f\"Down: {wrap_name(pw_down)}\"\n",
    "        )\n",
    "        ax.set_title(title, fontsize=13, pad=6)\n",
    "\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.tick_params(axis='x', labelsize=11)\n",
    "        ax.tick_params(axis='y', labelsize=11)\n",
    "        ax.legend_.remove()\n",
    "\n",
    "    for ax in axes[n:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    labels = [lab.capitalize() for lab in labels]\n",
    "\n",
    "    fig.legend(\n",
    "        handles, labels, title='Species',\n",
    "        ncol=2, loc='lower center',\n",
    "        bbox_to_anchor=(0.5, -0.02)\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Comparison of pathway with the highest difference in mean activity scores across {target_key} vs {context_key} liver cell types\",\n",
    "        fontsize=18, fontweight='bold'\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "    plt.savefig(save_path+save_key+\"pathways.pdf\", bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scSpecies_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
